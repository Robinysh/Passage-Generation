How to run:
  Run ./train.sh to train
  Run ./sample to generate bible
  See example (train.sh, sample.sh) and source code (train.py, sample.py) for further details.

Training general guidelines:

Hyperparameters obtained by trial and error:
  Number of layers: 2
  Learning rate decay rate: ~0.97

Learning rate: 0.002~0.0004

RNN typical size varies from 256 to 2048, depends on computational time and desired accuracy. The higher the better but takes a longer computational time.
 
Dropout typically ranges from 1 to 0.5. Amount indicates keep probability. Decrease to reduce overfitting.

Number of epoch typically ~50. Increase if loss function does not reach convergence.

Batch size typically ranges from 20 to 50. Increase to speedup performance and smoothes convergence.
